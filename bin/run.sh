#!/bin/bash

# ================= é…ç½®åŒº =================
# é»˜è®¤è·‘ä»Šå¤© (å› ä¸ºæ˜¯æµ‹è¯•ç¯å¢ƒ)ï¼Œç”Ÿäº§ç¯å¢ƒé€šå¸¸æ˜¯ `date -d "-1 day" +%F`
TARGET_DATE="${1:-$(date +%F)}"
HDFS_PATH="/origin_data/risk/dt=$TARGET_DATE"
# =========================================

echo "=================================================="
echo "   ğŸš€ å…¨é“¾è·¯ä»»åŠ¡å¯åŠ¨"
echo "   ğŸ“… ä¸šåŠ¡æ—¥æœŸ: $TARGET_DATE"
echo "   ğŸ“‚ HDFSè·¯å¾„: $HDFS_PATH"
echo "=================================================="

# --- Phase 0: å¯åŠ¨é‡‡é›†é€šé“ ---
echo ">>> [Phase 0] æ£€æŸ¥å¹¶å¯åŠ¨é‡‡é›†é€šé“..."
mkdir -p /root/risk_project/logs

# 1. å¯åŠ¨é€ æ•°è„šæœ¬
if pgrep -f mock_risk_data.py > /dev/null; then
    echo "    -> é€ æ•°è„šæœ¬å·²åœ¨è¿è¡Œï¼Œè·³è¿‡ã€‚"
else
    echo "    -> å¯åŠ¨ Python é€ æ•°è„šæœ¬..."
    cd /root/risk_project/code
    # å¼ºåˆ¶ä¼ å…¥æ—¥æœŸï¼Œä¿è¯ç”Ÿæˆçš„æ•°æ®å’Œå¤„ç†çš„æ—¥æœŸä¸€è‡´
    nohup python3 mock_risk_data.py --date "$TARGET_DATE" --interval 0.1 --count 2000 > /dev/null 2>&1 &
fi

# 2. å¯åŠ¨ Flume
if jps | grep Application > /dev/null; then
    echo "    -> Flume å·²åœ¨è¿è¡Œï¼Œè·³è¿‡ã€‚"
else
    echo "    -> å¯åŠ¨ Flume é‡‡é›†ä¸è½åœ°..."
    nohup /opt/module/flume/bin/flume-ng agent -n a1 -c conf -f /opt/module/flume/conf/file_to_kafka.conf -Dflume.root.logger=INFO,console > /root/risk_project/logs/flume_a1.log 2>&1 &
    nohup /opt/module/flume/bin/flume-ng agent -n a2 -c conf -f /opt/module/flume/conf/kafka_to_hdfs.conf -Dflume.root.logger=INFO,console > /root/risk_project/logs/flume_a2.log 2>&1 &
    
    echo "    -> Flume å·²å¯åŠ¨ï¼Œç­‰å¾… 60ç§’ è®©æ•°æ®è½ç›˜..."
    # å€’è®¡æ—¶æ•ˆæœ
    for i in {60..1}; do echo -ne "    â³ å‰©ä½™ $i ç§’...\r"; sleep 1; done
    echo ""
fi

# 3. å…³é”®æ£€æŸ¥ï¼šHDFS æ˜¯å¦çœŸçš„æœ‰æ•°æ®ï¼Ÿ
echo "    -> æ£€æŸ¥ HDFS æ•°æ®..."
hadoop fs -test -e $HDFS_PATH
if [ $? -ne 0 ]; then
    echo "âŒ [ERROR] HDFS è·¯å¾„ä¸å­˜åœ¨: $HDFS_PATH"
    echo "    å¯èƒ½æ˜¯ Flume æ²¡é…ç½®å¥½ dt= å‰ç¼€ï¼Œæˆ–è€…æ•°æ®æ²¡ç”Ÿæˆã€‚"
    echo "    è¯·æ£€æŸ¥ flume æ—¥å¿—ã€‚"
    exit 1
fi
echo "    -> âœ… HDFS æ•°æ®å°±ç»ªï¼"


# --- Phase 1: æŒ‚è½½ ODS ---
echo ">>> [Phase 1] æŒ‚è½½ ODS åˆ†åŒº..."
/opt/module/spark/bin/spark-sql -e "ALTER TABLE risk_db.ods_risk_log_inc ADD IF NOT EXISTS PARTITION (dt='$TARGET_DATE') LOCATION '$HDFS_PATH';"

# --- Phase 2: DWD æ¸…æ´— ---
echo ">>> [Phase 2] æäº¤ DWD æ¸…æ´—..."
/opt/module/spark/bin/spark-submit \
  --master yarn --deploy-mode client \
  --driver-memory 1g --executor-memory 1g --executor-cores 1 \
  /root/risk_project/code/dwd_risk_log.py "$TARGET_DATE"
if [ $? -ne 0 ]; then echo "âŒ DWD å¤±è´¥"; exit 1; fi

# --- Phase 3: DWS èšåˆ ---
echo ">>> [Phase 3] æäº¤ DWS èšåˆ..."
/opt/module/spark/bin/spark-submit \
  --master yarn --deploy-mode client \
  --driver-memory 1g --executor-memory 1g --executor-cores 1 \
  /root/risk_project/code/dws_user_profile.py "$TARGET_DATE"
if [ $? -ne 0 ]; then echo "âŒ DWS å¤±è´¥"; exit 1; fi

# --- Phase 4: ADS é£æ§ ---
echo ">>> [Phase 4] æäº¤ ADS é£æ§æŠ¥è¡¨..."
/opt/module/spark/bin/spark-submit \
  --master yarn --deploy-mode client \
  --driver-memory 1g --executor-memory 1g --executor-cores 1 \
  /root/risk_project/code/ads_risk_alert.py "$TARGET_DATE"
if [ $? -ne 0 ]; then echo "âŒ ADS å¤±è´¥"; exit 1; fi

# --- Phase 5: å¯¼å‡º MySQL ---
echo ">>> [Phase 5] å¯¼å‡ºé»‘åå•åˆ° MySQL..."
/opt/module/spark/bin/spark-submit \
  --master yarn --deploy-mode client \
  --driver-memory 1g --executor-memory 1g --executor-cores 1 \
  /root/risk_project/code/export_to_mysql.py "$TARGET_DATE"
if [ $? -ne 0 ]; then echo "âŒ å¯¼å‡ºå¤±è´¥"; exit 1; fi

echo "============================================================="
echo "âœ…âœ…âœ… å…¨é“¾è·¯è®¡ç®—å®Œæˆï¼è¯·å» DataGrip æŸ¥çœ‹ ads_black_list è¡¨ï¼"
echo "============================================================="
