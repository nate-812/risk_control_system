# -*- coding: utf-8 -*-
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# --- è·å–å¤–éƒ¨ä¼ å…¥çš„æ—¥æœŸå‚æ•° ---
if len(sys.argv) > 1:
    target_date = sys.argv[1]  # è·å–ç¬¬ä¸€ä¸ªå‚æ•°
else:
    print('æœªä¼ å…¥æ—¥æœŸå‚æ•°')

spark = SparkSession.builder \
    .appName(f"ADS_Risk_Alert{target_date}") \
    .enableHiveSupport() \
    .getOrCreate()

print(f">>> ğŸš€ å¼€å§‹æ‰§è¡Œ ADS é£æ§è§„åˆ™æ‰«æ ({target_date})...")

# --- æ ¸å¿ƒé£æ§ SQL ---
# è§„åˆ™ï¼šåŒä¸€ç”¨æˆ·ï¼Œè¿ç»­ 5 æ¬¡æ“ä½œçš„æ—¶é—´å·® < 1 ç§’
sql = f"""
SELECT DISTINCT user_id, 'é«˜é¢‘åˆ·å•' as risk_type, current_timestamp() as check_time
FROM (
    SELECT 
        user_id,
        event_time,
        LEAD(event_time, 4) OVER (PARTITION BY user_id ORDER BY event_time) as next_5th_time
    FROM risk_db.dwd_risk_log_inc
    WHERE dt = '{target_date}'
) t
-- åªè¦æ—¶é—´å·® <= 1ç§’ (å«0ç§’) éƒ½ç®—
WHERE (unix_timestamp(next_5th_time) - unix_timestamp(event_time)) <= 1
"""

df_black_list = spark.sql(sql)

print(">>> ğŸ˜± å‘ç°ç–‘ä¼¼é»‘äº§ç”¨æˆ·ï¼š")
df_black_list.show()

# å¢åŠ åˆ†åŒºåˆ— dt
df_final = df_black_list.withColumn("dt", lit(target_date))

print(">>> ğŸ’¾ å­˜å…¥ ADS è¡¨ (Parquet)...")
# å…³é”®ï¼šæ¨¡å¼è®¾ä¸º overwriteï¼Œæ ¼å¼ parquetï¼ŒæŒ‡å®šåˆ†åŒº
df_final.write \
    .mode("overwrite") \
    .format("parquet") \
    .partitionBy("dt") \
    .option("path", f"hdfs://master:8020/warehouse/risk/ads/ads_black_list/dt={target_date}") \
    .saveAsTable("risk_db.ads_black_list")