# -*- coding: utf-8 -*-
import sys  # <--- 1. å¼•å…¥ç³»ç»Ÿåº“
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# --- 2. è·å–å¤–éƒ¨ä¼ å…¥çš„æ—¥æœŸå‚æ•° ---
if len(sys.argv) > 1:
    target_date = sys.argv[1]  # è·å–ç¬¬ä¸€ä¸ªå‚æ•°
else:
    print('æœªä¼ å…¥æ—¥æœŸå‚æ•°')

print(f">>> ğŸš€ å¼€å§‹ DWD æ¸…æ´—ä»»åŠ¡ï¼Œä¸šåŠ¡æ—¥æœŸ: {target_date}")

# åˆå§‹åŒ– Spark
spark = SparkSession.builder \
    .appName(f"DWD_ETL_{target_date}") \
    .enableHiveSupport() \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# --- 3. è¯»å– ODS (ä½¿ç”¨å˜é‡è¿‡æ»¤) ---
# å…³é”®ç‚¹ï¼šè¯»çš„æ—¶å€™å°±è¦è¿‡æ»¤ï¼Œåªè¯»å½“å¤©çš„æ•°æ®ï¼
print(f">>> è¯»å– ODS å±‚æ•°æ® (dt={target_date})...")
df_ods = spark.sql(f"SELECT * FROM risk_db.ods_risk_log_inc WHERE dt='{target_date}'")
print(f"    - ODS è¯»å–æ¡æ•°: {df_ods.count()}")

# æ¸…æ´—é€»è¾‘
df_clean = df_ods \
    .filter("user_id IS NOT NULL") \
    .dropDuplicates(["user_id", "event_time", "item_id"])

print(f"    - æ¸…æ´—åæ¡æ•°: {df_clean.count()}")

# --- 4. å†™å…¥ DWD (ä½¿ç”¨å˜é‡åˆ†åŒº) ---
print(f">>> ğŸ’¾ å†™å…¥ DWD å±‚ (dt={target_date})...")

# ç»™æ•°æ®æ‰“ä¸Šå½“å¤©çš„æ—¥æœŸæ ‡ç­¾
df_final = df_clean.withColumn("dt", lit(target_date))

df_final.write \
    .mode("overwrite") \
    .format("parquet") \
    .partitionBy("dt") \
    .option("path", f"hdfs://master:8020/warehouse/risk/dwd/risk_log_inc/dt={target_date}") \
    .saveAsTable("risk_db.dwd_risk_log_inc")

print(">>> âœ… DWD å±‚æ„å»ºå®Œæˆï¼")
spark.stop()