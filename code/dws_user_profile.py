# -*- coding: utf-8 -*-
import sys  # <--- å¼•å…¥ç³»ç»Ÿåº“
from pyspark.sql import SparkSession

# --- å…³é”®ä¿®æ”¹å¼€å§‹ ---
# è·å–å‘½ä»¤è¡Œå‚æ•°
# sys.argv[0] æ˜¯è„šæœ¬æ–‡ä»¶åï¼Œsys.argv[1] æ˜¯ç¬¬ä¸€ä¸ªå‚æ•°
if len(sys.argv) > 1:
    target_date = sys.argv[1]
else:
    # å¦‚æœæ²¡ä¼ å‚æ•°ï¼Œç»™ä¸ªé»˜è®¤å€¼æ–¹ä¾¿æµ‹è¯•ï¼Œæˆ–è€…ç›´æ¥æŠ¥é”™é€€å‡º
    print('æœªä¼ å…¥æ—¥æœŸå‚æ•°')
# --- å…³é”®ä¿®æ”¹ç»“æŸ ---

spark = SparkSession.builder \
    .appName(f"DWS_User_Profile_{target_date}") \
    .enableHiveSupport() \
    .getOrCreate()

print(f">>> ğŸš€ å¼€å§‹æ„å»º DWS å±‚ç”»åƒï¼Œå¤„ç†æ—¥æœŸ: {target_date} ...")

# 1. ç¼–å†™ SQL èšåˆé€»è¾‘
# è¿™é‡Œçš„æ ¸å¿ƒæ˜¯ GROUP BY user_id
sql = f"""
    SELECT 
        user_id,
        '{target_date}' as dt,
        count(*) as total_actions,
        count(distinct ip_address) as unique_ip_count,
        min(event_time) as first_active_time,
        max(event_time) as last_active_time
    FROM risk_db.dwd_risk_log_inc
    WHERE dt = '{target_date}'
    GROUP BY user_id
"""

df_dws = spark.sql(sql)

print(f"    - èšåˆåçš„ç”¨æˆ·æ•°: {df_dws.count()}")

# 2. å†™å…¥ DWS è¡¨ (è‡ªåŠ¨å»ºè¡¨)
df_dws.write \
    .mode("overwrite") \
    .format("parquet") \
    .partitionBy("dt") \
    .option("path", f"hdfs://master:8020/warehouse/risk/dws/dws_user_profile/dt={target_date}") \
    .saveAsTable("risk_db.dws_user_profile")

print(">>> âœ… DWS å±‚æ„å»ºå®Œæˆï¼")
spark.stop()